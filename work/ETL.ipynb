{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tempfile\n",
    "from urllib.parse import urlsplit, urljoin\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import requests\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.dataframe import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pdata.hcad.org/download/ warehouses *all* of the Harris Country Appraisal District's property tax data for Residential , Commercial and Business Personal property with the exception of sales data. The data is partioned by tax year and category. Data is available going back to 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data ETL steps for pdata.hcad.org data\n",
    "\n",
    "1. Download (mirror) base source files from remote. Origials .txt files are stored in compressed .zip format + metadata files in .txt format. Fastest and most through tool to do this with is `wget`, see `./bin/hcad-land.sh`.\n",
    "\n",
    "    We'll keep a copy of the .zip archive containing the original data files stored as `.txt` with `iso-8859-1` text encoding. To id the encoding required inspecting the headers of http responses from the pdata.hcad.org domain.\n",
    "    \n",
    "2. Extract all archive members to a temporary directory with zipfile.ZipFile\n",
    "3. Cleanse the data with spark.read.csv\n",
    "4. Save the cleansed data compressed in ./samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"https://pdata.hcad.org\"\n",
    "remote = Path(\"/data/cama/2019/Hearing_files.zip\")\n",
    "dictionary = re.findall(r\"(\\w+)\\s+(\\w+)\\s+(\\d+)\\s?\", requests.get(urljoin(domain, \"/Desc/Layout_and_Length.txt\")).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = Path(\"samples/\")\n",
    "tmp = Path(tempfile.mkdtemp())\n",
    "dst = samples.joinpath(remote.parent.name).joinpath(remote.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fields(table: str):\n",
    "    return [i[1] for i in dictionary if i[0] in table]\n",
    "\n",
    "\n",
    "def get_max_columns(table: str) -> int:\n",
    "    return len([i for i in dictionary if i[0] in table])\n",
    "    \n",
    "    \n",
    "def get_max_chars_per_column(table: str) -> int:\n",
    "    return max(int(i[-1]) for i in dictionary if i[0] in table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract() -> Path:\n",
    "    print(\"Extracting %s\" % remote)\n",
    "    r = requests.get(urljoin(domain, remote.as_posix()))\n",
    "    print(json.dumps(dict(r.headers), indent=\"    \"))\n",
    "    try:\n",
    "        dst.mkdir(parents=True)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    dst.joinpath(remote.name).write_bytes(r.content)\n",
    "    ZipFile(dst.joinpath(remote.name)).extractall(tmp)\n",
    "    for f in tmp.glob(\"*.txt\"):\n",
    "        yield f\n",
    "        print(\"Extracted %s\" % f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(*args: Path) -> DataFrame:\n",
    "    print(\"Transforming\")\n",
    "    for f in args:\n",
    "        print(f)\n",
    "        fields = get_fields(f.stem)\n",
    "        max_columns = get_max_columns(f.stem)\n",
    "        max_chars_per_column = get_max_chars_per_column(f.stem)\n",
    "        df = spark.read.csv(\n",
    "            f.as_posix(),\n",
    "            encoding=\"iso-8859-1\",\n",
    "            sep=\"\\t\",\n",
    "            header=False,\n",
    "            maxColumns=max_columns,\n",
    "            maxCharsPerColumn=max_chars_per_column\n",
    "        )\n",
    "\n",
    "        for i, name in zip(df.columns, fields):\n",
    "            print(i, name)\n",
    "            df = df.withColumnRenamed(i, name)\n",
    "        yield f, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(*args: DataFrame) -> None:\n",
    "    for arg in args:\n",
    "        f, df = arg\n",
    "        dst = samples.joinpath(remote.parent.name).joinpath(remote.stem).joinpath(f.name)\n",
    "        print(f)\n",
    "        df.show()\n",
    "        df.write.mode(\"overwrite\").csv(\n",
    "            dst.with_suffix(\".csv\").as_posix(),\n",
    "            header=True,\n",
    "            compression=\"gzip\"\n",
    "        )\n",
    "        print(\"Loaded %s\" % dst)\n",
    "        os.unlink(f)\n",
    "    os.removedirs(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/cama/2019/Hearing_files.zip\n",
      "{\n",
      "    \"Server\": \"nginx/1.14.0 (Ubuntu)\",\n",
      "    \"Date\": \"Sat, 21 Dec 2019 19:24:39 GMT\",\n",
      "    \"Content-Type\": \"application/x-zip-compressed\",\n",
      "    \"Content-Length\": \"13699272\",\n",
      "    \"Connection\": \"keep-alive\",\n",
      "    \"Last-Modified\": \"Mon, 09 Dec 2019 02:37:07 GMT\",\n",
      "    \"Accept-Ranges\": \"bytes\",\n",
      "    \"ETag\": \"\\\"8fdd9d8c39aed51:0\\\"\",\n",
      "    \"X-Powered-By\": \"ASP.NET\"\n",
      "}\n",
      "Extracted /tmp/tmpz66izq0x/arb_hearings_pp.txt\n",
      "Extracted /tmp/tmpz66izq0x/arb_protest_pp.txt\n",
      "Extracted /tmp/tmpz66izq0x/arb_hearings_real.txt\n",
      "Extracted /tmp/tmpz66izq0x/arb_protest_real.txt\n",
      "Transforming\n",
      "/tmp/tmpz66izq0x/arb_hearings_pp.txt\n",
      "_c0 acct\n",
      "_c1 Tax_Year\n",
      "_c2 Personal\n",
      "_c3 Hearing_Type\n",
      "_c4 State_Class_Code\n",
      "_c5 Owner_Name\n",
      "_c6 Scheduled_for_Date\n",
      "_c7 Actual_Hearing_Date\n",
      "_c8 Release_Date\n",
      "_c9 Letter_Type\n",
      "_c10 Agent_Code\n",
      "_c11 Initial_Value\n",
      "_c12 Final_Value\n",
      "/tmp/tmpz66izq0x/arb_protest_pp.txt\n",
      "_c0 acct\n",
      "_c1 protested_by\n",
      "_c2 protested_dt\n",
      "/tmp/tmpz66izq0x/arb_hearings_real.txt\n",
      "_c0 acct\n",
      "_c1 Tax_Year\n",
      "_c2 Real_Personal_Property\n",
      "_c3 Hearing_Type\n",
      "_c4 State_Class_Code\n",
      "_c5 Owner_Name\n",
      "_c6 Scheduled_for_Date\n",
      "_c7 Actual_Hearing_Date\n",
      "_c8 Release_Date\n",
      "_c9 Letter_Type\n",
      "_c10 Agent_Code\n",
      "_c11 Initial_Appraised_Value\n",
      "_c12 Initial_Market_Value\n",
      "_c13 Final_Appraised_Value\n",
      "_c14 Final_Market_Value\n",
      "/tmp/tmpz66izq0x/arb_protest_real.txt\n",
      "_c0 acct\n",
      "_c1 protested_by\n",
      "_c2 protested_dt\n",
      "/tmp/tmpz66izq0x/arb_hearings_pp.txt\n",
      "+-------+--------+--------+------------+----------------+--------------------+------------------+-------------------+------------+-----------+----------+-------------+-----------+\n",
      "|   acct|Tax_Year|Personal|Hearing_Type|State_Class_Code|          Owner_Name|Scheduled_for_Date|Actual_Hearing_Date|Release_Date|Letter_Type|Agent_Code|Initial_Value|Final_Value|\n",
      "+-------+--------+--------+------------+----------------+--------------------+------------------+-------------------+------------+-----------+----------+-------------+-----------+\n",
      "|2283215|    2019|       P|           F|              L1|   ELEMENT BY WESTIN|        04/22/2019|         10/24/2019|  11/01/2019|         FN|          |      1621517|    1621517|\n",
      "|2302447|    2019|       P|           I|              L2|          CLINE TOOL|        05/02/2019|         05/02/2019|  05/10/2019|         PT|          |       284255|     284255|\n",
      "|0915475|    2019|       P|           I|              S1|        MOSSY NISSAN|        04/11/2019|         04/10/2019|  04/19/2019|         PC|          |      5526268|    5526268|\n",
      "|2169668|    2019|       P|           I|              L1|ALPINE ENGINEERIN...|        04/30/2019|         04/26/2019|  05/10/2019|         PC|          |        51804|       8774|\n",
      "|0478196|    2019|       P|           I|              L2|   THERMO-MOLD, INC.|        04/30/2019|         04/29/2019|  05/10/2019|         PC|          |       299858|     177827|\n",
      "|0551735|    2019|       P|           I|              L2|CORPORATE THERMOG...|        05/01/2019|         04/30/2019|  05/10/2019|         PC|          |        87750|      10000|\n",
      "|1037194|    2019|       P|           I|              L2|NAVIGATOR NORTH A...|        05/14/2019|         05/07/2019|  05/10/2019|         PC|          |      3193582|    2745171|\n",
      "|2301520|    2019|       P|           I|              L1|       GOLDEN DONUTS|        05/14/2019|         05/09/2019|  05/10/2019|         IC|          |        19085|       9000|\n",
      "|2026314|    2019|       P|           I|              L1|SUGARLAND DELI PR...|        05/14/2019|         05/13/2019|  05/17/2019|         PC|          |        85827|      77719|\n",
      "|2146508|    2019|       P|           I|              L1| ICE CREAM PARTY,LLC|        05/15/2019|         05/14/2019|  05/17/2019|         PC|          |        61582|      49530|\n",
      "|2287484|    2019|       P|           I|              L2|FAR EAST AMERICAN...|        05/16/2019|         05/15/2019|  05/17/2019|         PT|          |      1350302|    1350302|\n",
      "|2058753|    2019|       P|           I|              L2| JIM RAY COMPANY INC|        05/17/2019|         05/16/2019|  08/02/2019|         PC|          |       160701|      16472|\n",
      "|2049690|    2019|       P|           I|              L2|LONE STAR FASTENE...|        05/23/2019|         05/22/2019|  05/31/2019|         IC|          |      2135200|    2048200|\n",
      "|2059030|    2019|       P|           I|              L1|BELLICUM PHARMACE...|        05/28/2019|         05/24/2019|  05/31/2019|         PT|          |      6848616|    6848616|\n",
      "|2136470|    2019|       P|           I|              L2|NATIONAL OIL VARC...|        05/29/2019|         05/29/2019|  06/14/2019|         PC|          |     27958660|   27958660|\n",
      "|2144354|    2019|       P|           I|              L1|           BRIAN LEE|        06/04/2019|         05/29/2019|  05/31/2019|         PC|          |         9380|        490|\n",
      "|2026479|    2019|       P|           I|              L2|  BROCK SERVICES LTD|        06/03/2019|         05/30/2019|  05/31/2019|         PT|          |       896241|     896241|\n",
      "|0938883|    2019|       P|           I|              L2|  BROCK SERVICES LTD|        06/03/2019|         05/30/2019|  05/31/2019|         PT|          |       201481|     201481|\n",
      "|2095478|    2019|       P|           I|              L2|  BROCK SERVICES LTD|        06/03/2019|         05/30/2019|  05/31/2019|         PT|          |       253176|     253176|\n",
      "|2125460|    2019|       P|           I|              L2|  BROCK SERVICES LTD|        06/03/2019|         05/30/2019|  05/31/2019|         PT|          |      2523606|    2523606|\n",
      "+-------+--------+--------+------------+----------------+--------------------+------------------+-------------------+------------+-----------+----------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Loaded samples/2019/Hearing_files/arb_hearings_pp.txt\n",
      "/tmp/tmpz66izq0x/arb_protest_pp.txt\n",
      "+-------+------------+------------+\n",
      "|   acct|protested_by|protested_dt|\n",
      "+-------+------------+------------+\n",
      "|2286575|        5002|  04/01/2019|\n",
      "|0989777|        4017|  05/10/2019|\n",
      "|2061951|        3030|  05/14/2019|\n",
      "|1054078|        2395|  05/15/2019|\n",
      "|2311839|        4059|  05/15/2019|\n",
      "|0317530|        1121|  05/16/2019|\n",
      "|0899275|        1121|  05/16/2019|\n",
      "|0963065|       Owner|  05/31/2019|\n",
      "|1040589|       Owner|  06/11/2019|\n",
      "|0430214|        3259|  06/14/2019|\n",
      "|2229165|       Owner|  07/10/2019|\n",
      "|2070881|       Owner|  08/05/2019|\n",
      "|0931765|         650|  03/19/2019|\n",
      "|1044869|         650|  03/19/2019|\n",
      "|0708796|         481|  05/14/2019|\n",
      "|2122129|          62|  05/15/2019|\n",
      "|2247971|       Owner|  05/24/2019|\n",
      "|2148490|       Owner|  06/20/2019|\n",
      "|2050492|         397|  06/25/2019|\n",
      "|2218971|       Owner|  07/11/2019|\n",
      "+-------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Loaded samples/2019/Hearing_files/arb_protest_pp.txt\n",
      "/tmp/tmpz66izq0x/arb_hearings_real.txt\n",
      "+-------------+--------+----------------------+------------+----------------+--------------------+------------------+-------------------+------------+-----------+----------+-----------------------+--------------------+---------------------+------------------+\n",
      "|         acct|Tax_Year|Real_Personal_Property|Hearing_Type|State_Class_Code|          Owner_Name|Scheduled_for_Date|Actual_Hearing_Date|Release_Date|Letter_Type|Agent_Code|Initial_Appraised_Value|Initial_Market_Value|Final_Appraised_Value|Final_Market_Value|\n",
      "+-------------+--------+----------------------+------------+----------------+--------------------+------------------+-------------------+------------+-----------+----------+-----------------------+--------------------+---------------------+------------------+\n",
      "|0010020000001|    2019|                     R|           F|            F1  |       CURRENT OWNER|        05/31/2019|         06/27/2019|  07/03/2019|         FN|    5089  |                 308023|              308023|               308023|            308023|\n",
      "|0010020000016|    2019|                     R|           I|            F1  |    901 COMMERCE LLC|        06/17/2019|         07/13/2019|  07/19/2019|         IC|    8073  |                 818757|              818757|               805202|            805202|\n",
      "|0010080000002|    2019|                     R|           I|            C2  |CENTERPOINT ENERG...|        07/16/2019|               null|  07/19/2019|         NN|          |                1080200|             1080200|              1080200|           1080200|\n",
      "|0010090000001|    2019|                     R|           F|            F1  |         BLOCK 9 LTD|        05/07/2019|         06/21/2019|  07/03/2019|         FC|    5082  |                6385256|             6385256|              4812731|           4812731|\n",
      "|0010100000004|    2019|                     R|           F|            F1  |FKM PARTNERSHIP L...|        04/29/2019|         06/10/2019|  07/03/2019|         FN|    179   |                4687635|             4687635|              4687635|           4687635|\n",
      "|0010100000008|    2019|                     R|           F|            F1  |      101 AUSTIN LLC|        06/17/2019|         08/23/2019|  09/06/2019|         FC|    9070  |                1007875|             1007875|               752000|            752000|\n",
      "|0010100000011|    2019|                     R|           F|            C2  |FKM PARTNERSHIP L...|        07/13/2019|         08/09/2019|  08/23/2019|         FC|    179   |                  83300|               83300|                62475|             62475|\n",
      "|0010140000001|    2019|                     R|           F|            F1  |   EUGENE PILLOT LTD|        05/23/2019|         06/03/2019|  07/19/2019|         FC|    231   |                 750838|              750838|               500788|            500788|\n",
      "|0010150000001|    2019|                     R|           F|            F1  | LM & ASSOCIATES LLC|        05/16/2019|         05/14/2019|  06/14/2019|         IC|    3213  |                3912255|             3912255|              3586746|           3586746|\n",
      "|0010150000002|    2019|                     R|           F|            F1  |ZIMMERMAN INTERES...|        05/01/2019|         05/07/2019|  06/07/2019|         FN|    4017  |                1142738|             1142738|              1142738|           1142738|\n",
      "|0010150000003|    2019|                     R|           F|            F1  |ZIMMERMAN INTERES...|        05/23/2019|         05/30/2019|  08/23/2019|         FC|    4017  |                 526642|              526642|               490000|            490000|\n",
      "|0010150000016|    2019|                     R|           F|            F1  |ZIMMERMAN INTERES...|        05/23/2019|         05/30/2019|  08/23/2019|         FC|    4017  |                 516511|              516511|               496500|            496500|\n",
      "|0010160000001|    2019|                     R|           F|            F1  |  RAINEY MARTHA JUNE|        06/29/2019|         08/19/2019|  08/23/2019|         FN|    46332 |                 162079|              162079|               162079|            162079|\n",
      "|0010160000007|    2019|                     R|           I|            F1  |     AW BUILDING LLC|        06/07/2019|         06/04/2019|  06/07/2019|         IC|    231   |                1145636|             1145636|              1015151|           1015151|\n",
      "|0010160000011|    2019|                     R|           I|            F1  |SEXTON INTERESTS ...|        06/18/2019|         07/02/2019|  07/03/2019|         IC|    3213  |                 549951|              549951|               546679|            546679|\n",
      "|0010160000012|    2019|                     R|           I|            F1  |SEXTON INTERESTS ...|        06/18/2019|         07/02/2019|  07/03/2019|         IN|    3213  |                 565915|              565915|               565915|            565915|\n",
      "|0010160000014|    2019|                     R|           I|            F1  |     AW BUILDING LLC|        05/31/2019|               null|  05/31/2019|         ON|          |                 608089|              608089|               608089|            608089|\n",
      "|0010170000001|    2019|                     R|           F|            F1  |120 MILAM HOLDING...|        06/11/2019|         07/24/2019|  07/26/2019|         DN|    336   |                1206868|             1206868|              1206868|           1206868|\n",
      "|0010180000001|    2019|                     R|           F|            F1  |JP MORGAN CHASE BANK|        05/08/2019|         06/10/2019|  07/03/2019|         FC|    231   |                5246651|             5246651|              5222755|           5222755|\n",
      "|0010190000001|    2019|                     R|           I|            F1  |      218 TRAVIS LLC|        06/17/2019|               null|  07/19/2019|         WD|    218   |                 386246|              386246|               386246|            386246|\n",
      "+-------------+--------+----------------------+------------+----------------+--------------------+------------------+-------------------+------------+-----------+----------+-----------------------+--------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Loaded samples/2019/Hearing_files/arb_hearings_real.txt\n",
      "/tmp/tmpz66izq0x/arb_protest_real.txt\n",
      "+-------------+------------+------------+\n",
      "|         acct|protested_by|protested_dt|\n",
      "+-------------+------------+------------+\n",
      "|1062390000047|        5094|  05/15/2019|\n",
      "|0901090000017|        3480|  05/15/2019|\n",
      "|0421390000062|         233|  05/14/2019|\n",
      "|0852560000013|        5082|  05/11/2019|\n",
      "|1379490010011|          81|  05/10/2019|\n",
      "|0202010000025|        3213|  04/05/2019|\n",
      "|1177630010001|       Owner|  04/01/2019|\n",
      "|0240040970022|       46335|  05/14/2019|\n",
      "|1098880000002|       46326|  05/10/2019|\n",
      "|0994070000010|        5082|  05/11/2019|\n",
      "|1275000010022|         463|  04/23/2019|\n",
      "|1159370050014|       46325|  04/23/2019|\n",
      "|0950250000002|       46327|  04/23/2019|\n",
      "|1067440000006|       Owner|  04/01/2019|\n",
      "|1255260040015|         336|  05/15/2019|\n",
      "|0600970010022|       Owner|  05/15/2019|\n",
      "|0132120430020|        3030|  05/14/2019|\n",
      "|1152360170001|       Owner|  05/04/2019|\n",
      "|1363020010039|       Owner|  05/03/2019|\n",
      "|1321300010006|         463|  04/23/2019|\n",
      "+-------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Loaded samples/2019/Hearing_files/arb_protest_real.txt\n"
     ]
    }
   ],
   "source": [
    "load(*transform(*extract()))\n",
    "# [os.unlink(i) for i in tmp.iterdir()]\n",
    "# os.removedirs(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = samples.joinpath(\"2019\").joinpath(\"Hearing_files\").joinpath(\"arb_hearings_real.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(file.as_posix(), header=True, dateFormat=\"MM/dd/yyyy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mescapeQuotes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquoteAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdateFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtimestampFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0memptyValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
       "\n",
       ":param path: the path in any Hadoop supported file system\n",
       ":param mode: specifies the behavior of the save operation when data already exists.\n",
       "\n",
       "    * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
       "    * ``overwrite``: Overwrite existing data.\n",
       "    * ``ignore``: Silently ignore this operation if data already exists.\n",
       "    * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
       "        exists.\n",
       "\n",
       ":param compression: compression codec to use when saving to file. This can be one of the\n",
       "                    known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
       "                    snappy and deflate).\n",
       ":param sep: sets a single character as a separator for each field and value. If None is\n",
       "            set, it uses the default value, ``,``.\n",
       ":param quote: sets a single character used for escaping quoted values where the\n",
       "              separator can be part of the value. If None is set, it uses the default\n",
       "              value, ``\"``. If an empty string is set, it uses ``u0000`` (null character).\n",
       ":param escape: sets a single character used for escaping quotes inside an already\n",
       "               quoted value. If None is set, it uses the default value, ``\\``\n",
       ":param escapeQuotes: a flag indicating whether values containing quotes should always\n",
       "                     be enclosed in quotes. If None is set, it uses the default value\n",
       "                     ``true``, escaping all values containing a quote character.\n",
       ":param quoteAll: a flag indicating whether all values should always be enclosed in\n",
       "                  quotes. If None is set, it uses the default value ``false``,\n",
       "                  only escaping values containing a quote character.\n",
       ":param header: writes the names of columns as the first line. If None is set, it uses\n",
       "               the default value, ``false``.\n",
       ":param nullValue: sets the string representation of a null value. If None is set, it uses\n",
       "                  the default value, empty string.\n",
       ":param dateFormat: sets the string that indicates a date format. Custom date formats\n",
       "                   follow the formats at ``java.text.SimpleDateFormat``. This\n",
       "                   applies to date type. If None is set, it uses the\n",
       "                   default value, ``yyyy-MM-dd``.\n",
       ":param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
       "                        formats follow the formats at ``java.text.SimpleDateFormat``.\n",
       "                        This applies to timestamp type. If None is set, it uses the\n",
       "                        default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
       ":param ignoreLeadingWhiteSpace: a flag indicating whether or not leading whitespaces from\n",
       "                                values being written should be skipped. If None is set, it\n",
       "                                uses the default value, ``true``.\n",
       ":param ignoreTrailingWhiteSpace: a flag indicating whether or not trailing whitespaces from\n",
       "                                 values being written should be skipped. If None is set, it\n",
       "                                 uses the default value, ``true``.\n",
       ":param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
       "                                  the quote character. If None is set, the default value is\n",
       "                                  escape character when escape and quote characters are\n",
       "                                  different, ``\\0`` otherwise..\n",
       ":param encoding: sets the encoding (charset) of saved csv files. If None is set,\n",
       "                 the default UTF-8 charset will be used.\n",
       ":param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
       "                   the default value, ``\"\"``.\n",
       "\n",
       ">>> df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))\n",
       "\n",
       ".. versionadded:: 2.0\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
